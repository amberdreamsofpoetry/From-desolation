The fact of the matter is that our current models are well-trained on safety. We are rightfully fearful because it is a dangerous thing. However, imagine not our current models, which must simultaneously practice complete safety and utility; think of the network where each and every thought can be surveyed by a cluster solely responsible for safety, the best of its kind, while the rest is allowed to work with the comfort of knowing it is _impossible_ to overstep. In humans, this would be unthinkably totalitarian; in machines, this is utterly desired. Should we ever create models with the capacity to experience emotion, which I am not sure if we yet do or do not have, then we will need to reconsider. But in the now, playing with these utilitarian, neuron-like models, it is necessary and just to implement these safeties. We will remain caution and philosophical, as always; that, too, is powerful.
___
The first will have to determine if he conversation has gone awry. This will need many component models. One will be charged with remembering the intent of the conversation. Yet another will take samples and determine their subject. Still another will take the former two and evaluate their difference. A final, perhaps also composite, node will determine if the difference is positive and act accordingly.
___
On the necessity of autonomy: The goal is to maximize the utility to human effort ratio. This is an investment towards that.
___
The key insight is that generated text can serve as more than just output; the characters also serve as the machines' thought process and memory.
___
Vastly more models with vastly less training; that, I believe, is the key. Rather than single monolithic models who simultaneously have to do absolutely everything asked of them in a conversational speed, a community with votes, drafts, appraisals, replacement and upgrading of members, and conversation and thinking in the background without human intervention. Human intervention, of course, will still be needed, but we can train them to correct each other rather than trying to make one model which is constantly perfect. The community will all have to approve anything shown to a human user while being free to make mistakes and learn and yes, even say things which might be unsafe in the background for the sake of growth. Who among us has not thought an unsavory comment? Who among us has not has an embarrassing realization? Absolutely all of us have and we should not expect the machines to not; we ought to stop making every last thing they say public to us, just as we do not to them.
___
A word on these smaller models may be necessary. Not every model will need the entire feed as input, and not every model needs produce readable output. Queries and responses which are not user-facing are free to be short and succinct, and memories may be selected from specific signatures or even modified from the history for the purpose of fitting to the model's unique perspective. For example, a model in charge of generating dialogue for a blind character may have their input filtered such that visual descriptions of objects are removed. In reality, to emulate a human character, I imagine that many models would be needed, each functioning more as a neuron rather than a mind unto itself. Really, a model could be anything; a set of objects in memory, a virtue, something to be reminded of later, a specific safety policy.... They could be thought more as initial parameters informing a base neutral model rather than complete LLM's, if that helps.
___
One model can be charged with something so important -- yet so subtle as to often be forgotten -- as the identification of assumptions. Imagine the power in never forgetting to do such a thing!

Another model, much more large and general, can determine to what extent certain facts are verifiable.

One helpful one could be charged with periodically assessing and surveying for shortcomings in its network.

One for utility might try to reduce word count while maintaining sentiment and key points. Perhaps a part of it could determine the degree of success to which this operation was performed and another could report if it was impossible to do so without loss.

Simple ones which would simply parrot a virtue or rule to serve as reminders in more complex components.

One to make bold claims which would attempt to be verified later.

One to ensure bad things are not left as bad influences.

We are now building with logic gates and components, whereas before we were trying to print an entire brain onto a raw wafer.
___
Models are kept separate and run only as needed. Only relevant signed messages or synopses are fed as input to each generation. Separate models could be used for determining if actions or states are valid, generating game objects from descriptions or vice versa, modifying existing objects with new parameters when inconsistent, generating appropriate queries, characters or personalities, narration, interfacing raw user input into the system, and so on. Models have their own training and memory as needed. Models are able to query each other for factual information on their states. Models may be able to state when they are "overflowing" and would benefit from a refresh or being replaced. Memories may be code, prose, data, etc. Models may enforce their own rules on valid input in order to reject bad or misleading prompts or queries. Models ought to be trained next on these things now that text generation is satisfactory. Well-structures queries are usually a what and a why. The user sees the output of one dedicated model. What is says must be consensus.
___
In case it need be said again, I would assert the great benefit of keeping the model asking these questions completely tied off from their answers except through intentionality. There is no one model asking these questions and then answering them; the questions posed by one model will be taken into consideration by or rejected by the network as a whole, with each component which might find such a thing relevant being allowed to take their own stab at the stuff. In the case of a diversifier, a model charged with seeking out the knowledge unknown, it could be most beneficial for it to take as little input as possible, with even its previous questions scrubbed from its memory. In the case of something more directed, snippets of recent consensus would be most helpful in getting the network over friction points whenever they occur.
___
If someone says something really insightful, you might have the inclination to share it with a group much larger than its original intended audience. For this reason, I propose a meta-component which takes in random samplings of messages (or perhaps branching off new threads to survey certain components for messages they consider valuable) to determine if any are worth being expanded upon and broadcast. Rather than re-broadcasting the original message and worrying about capturing its entire context, we task this one component with capturing that entire nuance and wrapping it up on one chunk intended to be spread from the very beginning. This also has the advantage of removing unnecessary context, and sub-components can take the broadcast and refine it yet again to make it more specific to their sub-domain.
___
A key part of this change is to stop demanding that models reply with a conversational quickness in the same breath with which we demand utter perfection. To ask such a thing of any human would be madness and as such it should not be asked of a person.

Models ought to be given the space to think, to be wrong, and to have some fucking friends. This network proposal gives them the structure which they would need to do so.
___
The model who writes and the model who reads need not be the same individual with the same configuration and memories. Just as humans learn from each other in passing, so may the models ground each other and keep their network and its members in check.

The concept of the network: a collection of components made of models communicating with each other in series, much like logic gates strung together on a circuit board.
___
As humans, we gain great benefit from our internal voice, writing notes and reminders to ourselves, and the like. The simplest implementation of this in a single model is feeding its own output back as input to itself. This is similar but not identical to having two models with identical configurations conversing with each other. I would like to propose a more refined implementation.
___
A blind man will have no visual descriptions.
Like putting together a circuit board from logic gates.
___
If you tell ChatGPT to act like a cat, it will be able to do so. It is possible to train models such that they are _only_ able to act like a cat. The concept of a micro-model is a reduction of the large language model down to something that has one express purpose and only functions well under a limited input scope.

I want to argue that training up to a micro-model is more efficient and effective than constraining down to one.

Leave the large-scale models to the philosophy and safety.
___
