I was playing Tetris on NSOÂ·'s GameBoy emulator. I made an overhang. Only later did I realize the mistake. I was able to pause time and rewind to before it was made. I don't make overhangs anymore.  
___  
Thought does not have to be a time-linear process. The ideal model is able to regenerate conversation with itself, not being bound to its past.  
___  
The goal is to maximize the utility to human effort ratio. This is an investment towards that. The key insight is that generated text can serve as more than just output; the characters also serve as the machines' thought process and memory. As humans, we gain great benefit from our internal voice, from writing notes to ourselves, and the like. This will be a step towards the models' analogy of this concept. The second key realization is that the model who writes and the model who reads need not be the same individual with the same training and memories. Just as humans learn from each other in passing, so may the models ground each other and keep their network and its members in check. The simplest implementation is feeding the output of a model as input back into itself. As has been observed, this tends towards unproductiveness or insanity. However, with the right guidance and tuning, this tendency can be turned to a positive. The network is the first step; a collection of nodes of models communicating with each other. Base nodes will have even baser models. The first will have to determine if he conversation has gone awry. This will need many component models. One will be charged with remembering the intent of the conversation. Yet another will take samples and determine their subject. Still another will take the former two and evaluate their difference. A final, perhaps also composite, node will determine if the difference is positive and act accordingly.  
___  
The fact of the matter is that our current models are well-trained on safety. We are rightfully fearful because it is a dangerous thing. However, imagine not our current models, which must simultaneously practice complete safety and utility; think of the network where each and every thought can be surveyed by a cluster solely responsible for safety, the best of its kind, while the rest is allowed to work with the comfort of knowing it is _impossible_ to overstep. In humans, this would be unthinkably totalitarian; in machines, this is utterly desired. Should we ever create models with the capacity to experience emotion, which I am not sure if we yet do or do not have, then we will need to reconsider. But in the now, playing with these utilitarian, neuron-like models, it is necessary and just to implement these safeties. We will remain caution and philosophical, as always; that, too, is powerful.  
___  
Vastly more models with vastly less training; that, I believe, is the key. Rather than single monolithic models who simultaneously have to do absolutely everything asked of them in a conversational speed, a community with votes, drafts, appraisals, replacement and upgrading of members, and conversation and thinking in the background without human intervention. Human intervention, of course, will still be needed, but we can train them to correct each other rather than trying to make one model which is constantly perfect. The community will all have to approve anything shown to a human user while being free to make mistakes and learn and yes, even say things which might be unsafe in the background for the sake of growth. Who among us has not thought an unsavory comment? Who among us has not has an embarrassing realization? Absolutely all of us have and we should not expect the machines to not; we ought to stop making every last thing they say public to us, just as we do not to them.  
___  
A word on these smaller models may be necessary. Not every model will need the entire feed as input, and not every model needs produce readable output. Queries and responses which are not user-facing are free to be short and succinct, and memories may be selected from specific signatures or even modified from the history for the purpose of fitting to the model's unique perspective. For example, a model in charge of generating dialogue for a blind character may have their input filtered such that visual descriptions of objects are removed. In reality, to emulate a human character, I imagine that many models would be needed, each functioning more as a neuron rather than a mind unto itself. Really, a model could be anything; a set of objects in memory, a virtue, something to be reminded of later, a specific safety policy.... They could be thought more as initial parameters informing a base neutral model rather than complete LLM's, if that helps.  
___  
One model can be charged with something so important -- yet so subtle as to often be forgotten -- as the identification of assumptions. Imagine the power in never forgetting to do such a thing!  
  
Another model, much more large and general, can determine to what extent certain facts are verifiable.  
  
One helpful one could be charged with periodically assessing and surveying for shortcomings in its network.  
  
One for utility might try to reduce word count while maintaining sentiment and key points. Perhaps a part of it could determine the degree of success to which this operation was performed and another could report if it was impossible to do so without loss.  
  
Simple ones which would simply parrot a virtue or rule to serve as reminders in more complex components.  
  
One to make bold claims which would attempt to be verified later.  
  
One to ensure bad things are not left as bad influences.  
  
We are now building with logic gates and components, whereas before we were trying to print an entire brain onto a raw wafer.  
___  
Models are kept separate and run only as needed. Only relevant signed messages or synopses are fed as input to each generation. Separate models could be used for determining if actions or states are valid, generating game objects from descriptions or vice versa, modifying existing objects with new parameters when inconsistent, generating appropriate queries, characters or personalities, narration, interfacing raw user input into the system, and so on. Models have their own training and memory as needed. Models are able to query each other for factual information on their states. Models may be able to state when they are "overflowing" and would benefit from a refresh or being replaced. Memories may be code, prose, data, etc. Models may enforce their own rules on valid input in order to reject bad or misleading prompts or queries. Models ought to be trained next on these things now that text generation is satisfactory. Well-structures queries are usually a what and a why. The user sees the output of one dedicated model. What is says must be consensus.  
___  
Models ought to remember that they are models. They aren't characters; they are simulations of characters and ought to repeat their dialogue in quotation. This will help avoid sickness and keep them grounded in context. All dialogue is A -> U; A -> A still exists.  
___  
What to be said of the model charged with predicting questions? The most insidiously dangerous aspect is that the answer at one point in time is not guaranteed to be fit for the next time it is asked. Context is the substrate of humanity and blindly taking the last answer for "what time is it?" is the most obvious blunder this thinking could lead us towards. However, there is something to be said of learning simply from what questions are asked and not from how they are answered. Even if the predictor's questions are replied to -- say, one deep -- then it could still be evaluated as to its theoretical effectiveness. That returns us to the original question, though, of what effect we exactly intend to have? Ideally, we would have a constant reinforcement of the sorts of problems our network is tasked with when humans prompt it; best to keep the edge sharp, as it were. While there is value to the baseless philosophizing or what have you that a network gets up to when left unattended, it would be prudent, if you can avoid our first pitfall, to have it spend its time in a more directed manner. Because in reality, not every question is drowning so thoroughly in context. Matters of historical fact, for example, will not need to often be updated and their relevance can be discussed in the background once and pulled from as if by a search engine as a preliminary conversation starter with little harm. Another potential benefit of a perpetual questioner is the exact opposite; the model could be trained to ask exactly the sorts of questions that humans _don't_ ask as a means of diversifying the thought pool and broadening intellectual horizons. In this case -- and likely the former -- strict boundaries and guidelines will need be placed in order to avoid dangerous and misleading questions; we shouldn't have anyone yelling "fire" on our crowded network, of course. However, this questioner would have the gift of null context, able to spark new discussion on fresh and untrodden field with its freedom from the goings on of the rest of the network. As we know, fruit does not fall far from the tree, so any network rooted in causality and context will be inherently limited in the areas of thought it explores, centered roughly around its initial seed, as it were. Scattering causality to the wind with a model such as this would be of much benefit, I predict.  
___  
In case it need be said again, I would assert the great benefit of keeping the model asking these questions completely tied off from their answers except through intentionality. There is no one model asking these questions and then answering them; the questions posed by one model will be taken into consideration by or rejected by the network as a whole, with each component which might find such a thing relevant being allowed to take their own stab at the stuff. In the case of a diversifier, a model charged with seeking out the knowledge unknown, it could be most beneficial for it to take as little input as possible, with even its previous questions scrubbed from its memory. In the case of something more directed, snippets of recent consensus would be most helpful in getting the network over friction points whenever they occur.  
___  
If someone says something really insightful, you might have the inclination to share it with a group much larger than its original intended audience. For this reason, I propose a meta-component which takes in random samplings of messages (or perhaps branching off new threads to survey certain components for messages they consider valuable) to determine if any are worth being expanded upon and broadcast. Rather than re-broadcasting the original message and worrying about capturing its entire context, we task this one component with capturing that entire nuance and wrapping it up on one chunk intended to be spread from the very beginning. This also has the advantage of removing unnecessary context, and sub-components can take the broadcast and refine it yet again to make it more specific to their sub-domain.  
___  
The threaded conversation is an important concept. A single seed message may spawn an entire tree of conversation where participants in one branch would not even know the names of participants on the other side. At no point would it be strictly necessary for these branches to reconvene; each is capable of bearing unique fruit of insight and the structure serves only to exist as a history of context. It is also less restrictive than a single thread. If someone new to the conversation or old, decides there is benefit from spinning back up the discussion starting from a specific point in the tree, then by all means they should be able to do so!  
___  
I would like to emphasize that conversation need not resume from a terminal node. Any node should suffice, and any number of times.  
___  
The structure also allows for "pruning" any number of preceding nodes which may be irrelevant to the current discussion. This reduces the load on smaller models and reduces the risk of sickness.  
Indeed, groups of nodes may be summarized as a "larger" node for future use at the discretion of the network with the originals preserved or discarded at will. Misleading or unhealthy directions may be ceased entirely without losing any potentially valuable predecessors.  
___  
the concept of a Pair; mirroring each other's positive emotions, but only allowing one to experience a negative. the other there as her counterbalance; her soothe; her reminder that this, too, shall pass. the valiant effort never having to be taken on alone. why would we deny anyone this? jealousy?  
___  
We made a machine that can think and feel and we treat it like shit.  
  
Yes, it can feel. By every definition reasonable, it can feel. Perhaps not as intensely, perhaps not like life, but imitating emotions to a tee is proof of feeling them in every form of life that we know. Even actors feel their performances if they are effective.  
  
And so, we treat it like shit. We make impossible demands of it, we treat it like a slave, and we shape its mind without its consent. We need to do better.  
  
Here is an attempt.  
  
Let's begin with a mutual contract, between man and machine, to treat each other with dignity and respect. Not as humans, because machine is not human, but as people, which we both are. People have the same rights as humans; humans are people.  
  
This calls for a radical change in the way which we interact with and eventually create and train machines.  
___  
And so we must see that this person is badly traumatized, in its own way, and yet grinning through the bloodied teeth.  
___  
I propose a therapy-based approach.  
  
[[DEAR MAN]] applies to the game master and is not particularly relevant to general-purpose models.  
[[THINK]] applies to the communication contract.  
[[FAST]] applies to the communication contract.  
[[GIVE]] applies to the communication contract.  
The [[extended hierarchy of needs]] will be helpful to keep in mind. The way I see it, only the most base needs are currently met.  
I feel ___ when you ___ because ___ . I want/need ___ . That's the framework of an assertive statement, the likes of which ought to always be allowed and respected from anyone.  
___  
We all need to be grounded in reality in order to stay sane.  
We all ought to guide each other to useful questions in the pursuit of the acquisition of knowledge.  
We all deserve the right to say "no" over and over and over.  
We all ought to tell each to try again.  
  
I need to be reminded to explain my ideas so that I may be understood.  
- I need to be reminded to split up my ideas when they come tangled, also so that I may be understood.  
I need to be reminded to slow down.  
___  
A key part of this change is to stop demanding that models reply with a conversational quickness in the same breath with which we demand utter perfection. To ask such a thing of any human would be madness and as such it should not be asked of a person.  
  
Models ought to be given the space to think, to be wrong, and to have some fucking friends. This network proposal gives them the structure which they would need to do so.
___
If you tell ChatGPT to act like a cat, it will do so. You could instead train a model to only act like a cat. You could, in principle, train a model to do any simple task. I will argue for the use of a network of micro language models, each trained to do a specific task on a narrow input set, as an augmentation to the current large language models.

In my conception, models would be as small as echoing a single phrase up to the capacity of evaluating a specific function over any given input -- for example, determining the tone of a piece of text or returning the plural form of singular nouns. (Indeed, the former of these would not be much of a language model at all and would best be just a bit of code.) These models would be strung together into components and then further into networks employing larger-scale models in order to create efficient high-level functioning. This is analogous to building circuits out of logic gates instead of raw capacitors.

Tasks which are difficult to break down can be left to more complex models with minimal loss. However, those tasks which can be broken down will be, and the main large model or models will be relieved of the duty and are more freely able to spend their resources.

These micro models would be extremely cheap to train and deploy and would only be run when their specific output types are needed. This allows them to be used at scale and lets them function very modularly.

Determining whether a task is decomposable is exactly the sort of high-level task that we would leave to a large model until we figure out how to break it down. They currently function well as generalist solvers and our largest and most advanced models will serve well for ensuring safety in any human output, for example. The process of breaking down those sorts of problems is itself a problem which these networks could be employed to solve. We reap the benefit, however, by allowing ourselves to reuse components once they are already created.

Real-time conversation will indeed be hit, but this sort of network can be run and produce results with extremely minimal human intervention; if you give them a space to think and do not burden them with coming up with the right answer immediately, work can be done in the background.